{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Hillary Talks\n",
    "\n",
    "I am analysing the HCM dataset to identify trends in the foreign policy along the time. The work is based on the approach by Olalekan but extended to the whole dataset. It is also mainly in python instead of R as Olalekan code, but inserts R code in particular to use the R's countrycode package.\n",
    "\n",
    "The code below is far from being optimised and it is for \"playing\" purposes. It was based on python3 and used recent extensions for connecting to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "#http://blog.revolutionanalytics.com/2016/01/pipelining-r-python.html\n",
    "%load_ext rpy2.ipython\n",
    "import numpy, scipy, pandas\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import nltk, re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "conn = sqlite3.connect('../input/database.sqlite')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extraction of data (mails and dates) from SQLite to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".../lib/python3.4/site-packages/pandas/io/sql.py:1569: FutureWarning: frame_query is deprecated, use read_sql\n",
      "  warnings.warn(\"frame_query is deprecated, use read_sql\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas.io.sql as psql\n",
    "sql = \"SELECT SUBSTR(MetadataDateSent,1,7) as SentFOIA, ExtractedDateSent as Sent, ExtractedBodyText as EmailBody FROM Emails a WHERE ExtractedBodyText != '' AND MetadataDateSent != '' ORDER BY MetadataDateSent\"\n",
    "df = psql.frame_query(sql, conn)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Using and preparing R's countrycode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -o countrycode_data_without_atf\n",
    "library(countrycode)\n",
    "data(countrycode_data)\n",
    "#we assume that a country could be referred to as its full name or its ISO3 code\n",
    "#ec:: OJO with above ^^\n",
    "#as abbreviation so we get the list of countries\n",
    "#ec:: list of country names that will be not considered, including USA\n",
    "#ec:: USA (255) is excluded because we are interested only in foreign policy\n",
    "countrycode_data_without_atf <- countrycode_data[-c(255,102,83,5,6,9,16,23,31,32,34,36,66,87,88,98,104,105,106,143,117,122,130,154,155,177,183,188,191,201,202,203,204,205,206,207,209,210,212,215,218,224,230,241,248,249,250,266),]\n",
    "\n",
    "\n",
    "\n",
    "## ec:: the only accepted countries\n",
    "## ec:: as we will see later, regex is a prepared dataset that includes a pattern to find country.name (NICE!!!)\n",
    "#countries <- countrycode_data_without_atf[, c(\"country.name\", \"regex\", \"iso2c\", \"iso3c\")]\n",
    "#countries$other <- NA\n",
    "## ec:: not clear for me why UK is left apart as \"other\"\n",
    "#countries[countries$country.name==\"United Kingdom\",]$other <- \"UK\"\n",
    "##head(countrycode_data_without_atf)\n",
    "##head(countries)\n",
    "##countries[countries$country.name==\"United Kingdom\",]\n",
    "##countrycode_data_without_atf[with(countrycode_data_without_atf,  grepl(\"Russia\", country.name)),]\n",
    "##countries[countries$country.name==\"Saint Pierre and Miquelon\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Parsing the messages in the pandas file to identify possible undesired words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentFOIA</th>\n",
       "      <th>Sent</th>\n",
       "      <th>EmailBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 2009-01</td>\n",
       "      <td> Monday, December 21, 2009 2:33 PM</td>\n",
       "      <td> folks, i've attached stopword latest draft sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2009-03</td>\n",
       "      <td> Thursday, March 19, 2009 12:24 PM</td>\n",
       "      <td> talking points stopword secretary's call stopw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 2009-03</td>\n",
       "      <td>                                  </td>\n",
       "      <td> cheryl mills &lt;cherylmill5 friday, march 20, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 2009-03</td>\n",
       "      <td>     Friday, March 20, 20098:05 AM</td>\n",
       "      <td> i've printed stopword stopword stopword give s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 2009-03</td>\n",
       "      <td> Saturday, March 21, 2009 11:21 AM</td>\n",
       "      <td> story stopword stopword stopword good stopword...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SentFOIA                               Sent  \\\n",
       "0  2009-01  Monday, December 21, 2009 2:33 PM   \n",
       "1  2009-03  Thursday, March 19, 2009 12:24 PM   \n",
       "2  2009-03                                      \n",
       "3  2009-03      Friday, March 20, 20098:05 AM   \n",
       "4  2009-03  Saturday, March 21, 2009 11:21 AM   \n",
       "\n",
       "                                           EmailBody  \n",
       "0  folks, i've attached stopword latest draft sto...  \n",
       "1  talking points stopword secretary's call stopw...  \n",
       "2  cheryl mills <cherylmill5 friday, march 20, 20...  \n",
       "3  i've printed stopword stopword stopword give s...  \n",
       "4  story stopword stopword stopword good stopword...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sw = '|'.join(nltk.corpus.stopwords.words('english') + [\"re\", \"fm\", \"tv\", \"la\", \"al\", \"ben\", \"aq\"])\n",
    "#print(sw)\n",
    "#df_countries.loc[df_countries['country.name']=='Israel',]\n",
    "def sw(x):\n",
    "    x = x.split()\n",
    "    for i,w in enumerate(x):\n",
    "        #if w in [wrds for wrds in nltk.corpus.words.words() if len(w) <= 3] + [\"re\", \"fm\", \"tv\", \"la\", \"al\", \"ben\", \"aq\"]:\n",
    "        if w in nltk.corpus.stopwords.words('english') + [\"us\", \"il\", \"isr\", \"re\", \"fm\", \"tv\", \"la\", \"al\", \"ben\", \"aq\"]:\n",
    "            x[i] = 'stopword'\n",
    "    return ' '.join(x)\n",
    "\n",
    "df['EmailBody'] = df['EmailBody'].str.lower()\n",
    "df['EmailBody'] = df['EmailBody'].apply(sw)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Generation of a list to extract countries found at eac EmailBody\n",
    "The list is organised by position in the original dataset and consist in a tuple of country name, continent and region per location found at each EmailBody.\n",
    "In this case, only one record of the country is included if found at least one time in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [('Korea', 'NA', 'NA'),\n",
       "  (\"Korea, Democratic People's Republic of\", 'Asia', 'Eastern Asia')],\n",
       " [('Senegal', 'Africa', 'Western Africa')],\n",
       " [],\n",
       " [('Angola', 'Africa', 'Middle Africa'),\n",
       "  ('Japan', 'Asia', 'Eastern Asia'),\n",
       "  ('Madagascar', 'Africa', 'Eastern Africa'),\n",
       "  ('Morocco', 'Africa', 'Northern Africa')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('Mexico', 'Americas', 'Central America')],\n",
       " [('Mexico', 'Americas', 'Central America')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_countries = pandas2ri.ri2py(countrycode_data_without_atf)\n",
    "def match_regex(string, prog):\n",
    "    return re.match(prog, string)\n",
    "\n",
    "foundpattern = []\n",
    "for i, rowdf in df.iterrows():\n",
    "    p = []\n",
    "    splitrowdf = rowdf['EmailBody'].split()\n",
    "    for j,rowc in df_countries.iterrows():\n",
    "        #first find pattern\n",
    "        prog = re.compile(rowc['regex'])\n",
    "        if match_regex(rowdf['EmailBody'], prog):\n",
    "            p.append((rowc['country.name'], rowc['continent'], rowc['region']))\n",
    "            continue\n",
    "        elif rowdf['EmailBody'].find(rowc['country.name'].lower()) != -1:\n",
    "            p.append((rowc['country.name'], rowc['continent'], rowc['region']))\n",
    "            continue\n",
    "        elif rowc['iso2c'].lower() in splitrowdf:\n",
    "            p.append((rowc['country.name'], rowc['continent'], rowc['region']))\n",
    "            continue\n",
    "        elif rowc['iso2c'].lower() == 'gb' and 'uk' in splitrowdf:\n",
    "            p.append((rowc['country.name'], rowc['continent'], rowc['region']))\n",
    "            continue\n",
    "        elif rowc['iso3c'].lower() in splitrowdf:\n",
    "            p.append((rowc['country.name'], rowc['continent'], rowc['region']))\n",
    "            continue\n",
    "    foundpattern.append(p)\n",
    "foundpattern[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Counting Countries, Continents and Regions: Total\n",
    "\n",
    "Totals of the number of references to countries, continents and regions for all the messages. If several countries of similar region/continent are found in the same message, those regions and continents are counted only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countrycounter = Counter()\n",
    "continentcounter = Counter()\n",
    "regioncounter = Counter()\n",
    "for elem in foundpattern:\n",
    "    if elem == []: continue\n",
    "    donecontinentorregion = set()\n",
    "    for e in elem:\n",
    "        countrycounter.update([e[0]])\n",
    "        if e[1] not in donecontinentorregion:\n",
    "            continentcounter.update([e[1]])\n",
    "            donecontinentorregion.update([e[1]])\n",
    "        if e[2] not in donecontinentorregion:\n",
    "            regioncounter.update([e[2]])\n",
    "            donecontinentorregion.update([e[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Israel': 115, 'Afghanistan': 110, 'China': 97, 'Pakistan': 89, 'Oman': 85, 'Iraq': 79, 'India': 75, 'Peru': 70, 'Libya': 70, 'Angola': 65, 'Korea': 57, 'Germany': 47, 'United Kingdom': 46, 'Egypt': 45, 'Mexico': 40, 'Bangladesh': 39, \"Korea, Democratic People's Republic of\": 39, 'Brazil': 38, 'Jamaica': 37, 'Turkey': 37, 'Indonesia': 36, 'Japan': 35, 'Ireland': 33, 'France': 29, 'Mali': 28, 'Jordan': 27, 'Honduras': 27, 'Mauritania': 26, 'Morocco': 26, 'Armenia': 23, 'Australia': 23, 'Poland': 22, 'Colombia': 22, 'Cuba': 22, 'Qatar': 22, 'Guyana': 21, 'Panama': 21, 'Canada': 20, 'Ethiopia': 20, 'South Africa': 19, 'Singapore': 19, 'Spain': 18, 'Kenya': 16, 'Yemen': 16, 'Sudan': 15, 'Virgin Islands, U.S.': 15, 'Korea, Republic of': 14, 'Montserrat': 14, 'Sri Lanka': 14, 'Chile': 14, 'Saudi Arabia': 13, 'Estonia': 13, 'Argentina': 13, 'Congo': 12, 'Norway': 12, 'Puerto Rico': 11, 'Nicaragua': 11, 'Italy': 11, 'Yemen Arab Republic': 11, 'Georgia': 10, 'Viet Nam': 10, 'Liechtenstein': 10, 'Greece': 10, 'Sweden': 10, 'Niger': 10, 'Netherlands Antilles': 10, 'Ukraine': 9, 'Kuwait': 9, 'Czech Republic': 9, 'Nigeria': 9, 'Yugoslavia': 9, \"Yemen People's Republic\": 9, 'Northern Mariana Islands': 9, 'Costa Rica': 8, 'Somalia': 8, 'Ecuador': 8, 'Azerbaijan': 8, 'Lithuania': 8, 'Senegal': 8, 'Dominica': 8, 'Uruguay': 8, 'Romania': 7, 'Holy See (Vatican City State)': 7, 'Liberia': 7, 'Malaysia': 7, 'Portugal': 7, 'Serbia': 7, 'Dominican Republic': 7, 'Netherlands': 7, 'Thailand': 7, 'Lebanon': 7, 'Tunisia': 7, 'Congo, the Democratic Republic of the': 7, 'Faroe Islands': 6, 'Gabon': 6, 'South Sudan': 6, 'Uganda': 6, 'Russian Federation': 6, 'Myanmar': 6, 'Hungary': 6, 'Belgium': 6, 'Austria': 6, \"Cote d'Ivoire\": 6, 'Cameroon': 5, 'Switzerland': 5, 'Guinea': 5, 'Iceland': 5, 'Namibia': 5, 'Rwanda': 5, 'Philippines': 5, 'Algeria': 5, 'Cyprus': 5, 'Sierra Leone': 5, 'Hong Kong': 4, 'Zanzibar': 4, 'Slovakia': 4, 'Latvia': 4, 'Suriname': 4, 'Palestine, State of': 4, 'Cook Islands': 4, 'Trinidad and Tobago': 4, 'Baden': 4, 'Virgin Islands, British': 4, 'Ghana': 4, 'Hanover': 4, 'New Zealand': 4, 'Mongolia': 4, 'Republic of Vietnam': 4, 'Modena': 4, 'Cambodia': 4, 'Slovenia': 4, 'Albania': 3, 'Kazakhstan': 3, 'Uzbekistan': 3, 'Zambia': 3, 'Denmark': 3, 'Cocos (Keeling) Islands': 3, 'Bahrain': 3, 'New Caledonia': 3, 'Timor-Leste': 3, 'Zimbabwe': 3, 'Gambia': 3, 'Cabo Verde': 3, 'Barbados': 3, 'Guatemala': 3, 'Bermuda': 3, 'El Salvador': 3, 'Monaco': 3, 'Croatia': 2, 'Mauritius': 2, 'Guinea-Bissau': 2, 'Greenland': 2, 'Chad': 2, 'Papua New Guinea': 2, 'Vanuatu': 2, 'Luxembourg': 2, 'Turkmenistan': 2, 'Reunion': 2, 'Bhutan': 2, 'Malawi': 2, 'Eritrea': 2, 'Brunei Darussalam': 2, 'Madagascar': 2, 'Macao': 2, 'Kyrgyzstan': 2, 'Maldives': 2, 'Samoa': 2, 'Finland': 2, 'Nauru': 2, 'Iran, Islamic Republic of': 1, 'Marshall Islands': 1, 'Tajikistan': 1, 'Malta': 1, 'Belarus': 1, 'Gibraltar': 1, 'Kiribati': 1, 'United Arab Emirates': 1, 'Lesotho': 1, 'Anguilla': 1, 'Western Sahara': 1, 'Aland Islands': 1, 'Guam': 1, 'Burkina Faso': 1, 'Burundi': 1, 'Mozambique': 1, 'Bahamas': 1, 'Tonga': 1, 'Nepal': 1, 'Antigua and Barbuda': 1, 'Paraguay': 1, 'Comoros': 1, 'Czechoslovakia': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countrycounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Asia': 616, 'Africa': 356, 'Americas': 353, 'Europe': 254, 'NA': 73, 'Oceania': 52})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continentcounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Western Asia': 331, 'Southern Asia': 241, 'South America': 172, 'Eastern Asia': 152, 'Northern Africa': 151, 'Northern Europe': 126, 'Caribbean': 121, 'Central America': 102, 'Western Africa': 101, 'Middle Africa': 92, 'South-Eastern Asia': 88, 'Western Europe': 81, 'Eastern Africa': 67, 'Southern Europe': 59, 'Eastern Europe': 56, 'Southern Africa': 25, 'Northern America': 25, 'Australia and New Zealand': 24, 'Micronesia': 14, 'Central Asia': 10, 'Melanesia': 7, 'Polynesia': 7})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regioncounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Counting Countries, Continents and Regions per Year-Month\n",
    "\n",
    "A similar counting as above but grouped by year-month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-01-01 00:12:00\n",
      "2009-01-01 00:01:00\n",
      "2009-12-01 00:00:00\n",
      "2010-01-01 00:00:00\n",
      "1826\n"
     ]
    }
   ],
   "source": [
    "maxtime = datetime.datetime.strptime(df.SentFOIA.max(),\"%Y-%M\")\n",
    "print(maxtime)\n",
    "mintime = datetime.datetime.strptime(df.SentFOIA.min(),\"%Y-%M\")\n",
    "print(mintime)\n",
    "#http://docs.scipy.org/doc/numpy/reference/arrays.datetime.html\n",
    "#https://pymotw.com/2/datetime/\n",
    "#http://stackoverflow.com/questions/4130922/how-to-increment-datetime-by-custom-months-in-python-without-using-library\n",
    "current = datetime.datetime(mintime.year, 12, 1)\n",
    "print(current)\n",
    "next_month = datetime.datetime(mintime.year + int(12 / 12), (int(12 % 12) + 1), 1)\n",
    "print(next_month)\n",
    "dates = [mintime]\n",
    "end = 0\n",
    "print((maxtime-mintime).days)\n",
    "while end==0:\n",
    "    if (maxtime - next_month).days <= 31:\n",
    "        end = 1\n",
    "    datetocheck = dates[-1]\n",
    "    current = datetime.datetime(datetocheck.year, datetocheck.month, 1)\n",
    "    next_month = datetime.datetime(datetocheck.year + int(datetocheck.month / 12), (int(datetocheck.month % 12) + 1), 1)\n",
    "    dates.append(next_month)\n",
    "dates\n",
    "\n",
    "\n",
    "\n",
    "dateddata = dict([(d,[Counter(),Counter(),Counter()]) for d in dates])\n",
    "\n",
    "\n",
    "for ii, d in enumerate(df.SentFOIA):\n",
    "    elem = foundpattern[ii]\n",
    "    #print(elem)\n",
    "    current = datetime.datetime(int(d[:4]),int(d[5:7]),1)\n",
    "    if elem == []: continue\n",
    "    donecontinentorregion = set()\n",
    "    for e in elem:\n",
    "        #print(e)\n",
    "        dateddata[current][0].update([e[0]])\n",
    "        if e[1] not in donecontinentorregion:\n",
    "            dateddata[current][1].update([e[1]])\n",
    "            donecontinentorregion.update([e[1]])\n",
    "        if e[2] not in donecontinentorregion:\n",
    "            dateddata[current][2].update([e[2]])\n",
    "            donecontinentorregion.update([e[2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick report of the top 5 countries in the messaging at each year-month, ordered by number of mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01 00:01:00 ----- []\n",
      "2009-02-01 00:00:00 ----- []\n",
      "2009-03-01 00:00:00 ----- ['Mexico', 'Morocco', 'Palestine, State of', 'Oman', 'Senegal', 'China']\n",
      "2009-04-01 00:00:00 ----- ['Peru', 'Afghanistan', 'Oman', 'China', 'Brazil', 'Korea']\n",
      "2009-05-01 00:00:00 ----- ['Korea', 'China', 'Angola', \"Korea, Democratic People's Republic of\", 'Indonesia', 'Sri Lanka']\n",
      "2009-06-01 00:00:00 ----- ['Honduras', 'China', 'Cuba', 'Angola', 'Korea', 'Korea, Republic of']\n",
      "2009-07-01 00:00:00 ----- ['Korea', 'India', 'Angola', 'China', 'Peru', 'Korea, Republic of']\n",
      "2009-08-01 00:00:00 ----- ['Honduras', 'Turkey', 'China', 'Peru', 'Congo', 'India']\n",
      "2009-09-01 00:00:00 ----- ['Iraq', 'Afghanistan', 'Pakistan', 'China', 'Korea', 'Bangladesh']\n",
      "2009-10-01 00:00:00 ----- ['Sri Lanka', 'Pakistan', 'Afghanistan', 'Iraq', 'Peru', 'Israel']\n",
      "2009-11-01 00:00:00 ----- ['Afghanistan', 'Honduras', 'Peru', 'Iraq', 'Pakistan', 'China']\n",
      "2009-12-01 00:00:00 ----- ['Brazil', 'China', 'Japan', 'Peru', 'India', 'Afghanistan']\n",
      "2010-01-01 00:00:00 ----- ['China', 'Jamaica', 'Oman', 'Pakistan', 'Indonesia', 'Mauritania']\n",
      "2010-02-01 00:00:00 ----- ['Oman', 'Angola', 'Israel', 'Ireland', 'Lithuania', 'Afghanistan']\n",
      "2010-03-01 00:00:00 ----- ['Pakistan', 'Israel', 'Ireland', 'Afghanistan', 'China', 'United Kingdom']\n",
      "2010-04-01 00:00:00 ----- ['Pakistan', 'United Kingdom', 'Afghanistan', 'Oman', 'Poland', 'Germany']\n",
      "2010-05-01 00:00:00 ----- ['Israel', 'Korea', \"Korea, Democratic People's Republic of\", 'Oman', 'Jordan', 'India']\n",
      "2010-06-01 00:00:00 ----- ['Israel', 'Afghanistan', 'Angola', 'Pakistan', 'Iraq', 'Bangladesh']\n",
      "2010-07-01 00:00:00 ----- ['Afghanistan', 'Pakistan', 'Israel', 'Germany', 'Saudi Arabia', 'Cuba']\n",
      "2010-08-01 00:00:00 ----- ['Israel', 'Peru', 'Pakistan', 'India', 'Jamaica', 'Oman']\n",
      "2010-09-01 00:00:00 ----- ['Israel', 'India', 'Iraq', 'China', 'Germany', 'Afghanistan']\n",
      "2010-10-01 00:00:00 ----- ['Israel', 'Oman', 'China', 'Pakistan', 'Iraq', 'Peru']\n",
      "2010-11-01 00:00:00 ----- ['Jamaica', 'Oman', 'Turkey', 'India', 'Afghanistan', 'Egypt']\n",
      "2010-12-01 00:00:00 ----- ['Israel', 'Pakistan', 'Jamaica', 'Korea', 'Afghanistan', 'Oman']\n",
      "2011-01-01 00:00:00 ----- []\n",
      "2011-02-01 00:00:00 ----- []\n",
      "2011-03-01 00:00:00 ----- ['Libya', 'France', 'Egypt', 'Israel', 'Mongolia', 'Iraq']\n",
      "2011-04-01 00:00:00 ----- ['United Kingdom', 'Egypt', 'Libya', 'Panama', 'Cocos (Keeling) Islands', 'Jordan']\n",
      "2011-05-01 00:00:00 ----- ['Libya']\n",
      "2011-06-01 00:00:00 ----- ['United Kingdom']\n",
      "2011-07-01 00:00:00 ----- []\n",
      "2011-08-01 00:00:00 ----- ['India', 'Egypt', 'Australia', 'South Africa', 'Angola', 'Libya']\n",
      "2011-09-01 00:00:00 ----- ['Libya']\n",
      "2011-10-01 00:00:00 ----- []\n",
      "2011-11-01 00:00:00 ----- []\n",
      "2011-12-01 00:00:00 ----- []\n",
      "2012-01-01 00:00:00 ----- ['Libya', 'Ethiopia', 'Qatar']\n",
      "2012-02-01 00:00:00 ----- ['Libya']\n",
      "2012-03-01 00:00:00 ----- ['France', 'Libya', 'United Kingdom', 'Mali']\n",
      "2012-04-01 00:00:00 ----- ['Libya', 'Qatar', 'Switzerland', 'Cocos (Keeling) Islands', 'Italy', 'China']\n",
      "2012-05-01 00:00:00 ----- []\n",
      "2012-06-01 00:00:00 ----- []\n",
      "2012-07-01 00:00:00 ----- ['Libya', 'Armenia']\n",
      "2012-08-01 00:00:00 ----- ['Israel']\n",
      "2012-09-01 00:00:00 ----- ['Libya', 'Egypt', 'Yemen', 'Tunisia', 'Afghanistan', 'Morocco']\n",
      "2012-10-01 00:00:00 ----- ['Libya', 'Egypt', 'Faroe Islands', 'Papua New Guinea', 'Liechtenstein', 'Israel']\n",
      "2012-11-01 00:00:00 ----- ['Libya', 'Iraq', 'Egypt', 'Afghanistan', 'Estonia']\n",
      "2012-12-01 00:00:00 ----- ['Libya', 'France', 'Turkey']\n",
      "2013-01-01 00:00:00 ----- []\n",
      "2013-02-01 00:00:00 ----- []\n",
      "2013-03-01 00:00:00 ----- []\n",
      "2013-04-01 00:00:00 ----- []\n",
      "2013-05-01 00:00:00 ----- []\n",
      "2013-06-01 00:00:00 ----- []\n",
      "2013-07-01 00:00:00 ----- []\n",
      "2013-08-01 00:00:00 ----- []\n",
      "2013-09-01 00:00:00 ----- []\n",
      "2013-10-01 00:00:00 ----- []\n",
      "2013-11-01 00:00:00 ----- []\n",
      "2013-12-01 00:00:00 ----- []\n",
      "2014-01-01 00:00:00 ----- []\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#sorted(dateddata[list(dateddata)[4]][0].items(), key=operator.itemgetter(1), reverse=True)\n",
    "for elem in sorted(list(dateddata)):\n",
    "    #my_list.sort(key=operator.itemgetter(1))\n",
    "    print(elem,'-----',[x for x in [x[0] for i,x in enumerate(sorted(dateddata[elem][0].items(), key=operator.itemgetter(1), reverse=True)) if i <= 5]])\n",
    "    #print(elem,'-----',[x for x in list(dateddata[elem][0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as above per continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01 00:01:00 ----- []\n",
      "2009-02-01 00:00:00 ----- []\n",
      "2009-03-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'NA']\n",
      "2009-04-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA']\n",
      "2009-05-01 00:00:00 ----- ['Asia', 'Africa', 'Americas', 'Europe', 'NA', 'Oceania']\n",
      "2009-06-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2009-07-01 00:00:00 ----- ['Asia', 'Africa', 'Americas', 'Europe', 'NA', 'Oceania']\n",
      "2009-08-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2009-09-01 00:00:00 ----- ['Asia', 'Africa', 'Europe', 'Americas', 'NA', 'Oceania']\n",
      "2009-10-01 00:00:00 ----- ['Asia', 'Africa', 'Europe', 'Americas', 'Oceania', 'NA']\n",
      "2009-11-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2009-12-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'Oceania', 'NA']\n",
      "2010-01-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'Oceania', 'NA']\n",
      "2010-02-01 00:00:00 ----- ['Asia', 'Americas', 'Europe', 'Africa', 'Oceania', 'NA']\n",
      "2010-03-01 00:00:00 ----- ['Asia', 'Americas', 'Europe', 'Africa', 'Oceania', 'NA']\n",
      "2010-04-01 00:00:00 ----- ['Asia', 'Europe', 'Africa', 'Americas', 'NA']\n",
      "2010-05-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2010-06-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2010-07-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe']\n",
      "2010-08-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2010-09-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'Oceania', 'NA']\n",
      "2010-10-01 00:00:00 ----- ['Asia', 'Americas', 'Africa', 'Europe', 'NA', 'Oceania']\n",
      "2010-11-01 00:00:00 ----- ['Asia', 'Europe', 'Americas', 'Africa', 'Oceania']\n",
      "2010-12-01 00:00:00 ----- ['Asia', 'Americas', 'Europe', 'Africa', 'NA', 'Oceania']\n",
      "2011-01-01 00:00:00 ----- []\n",
      "2011-02-01 00:00:00 ----- []\n",
      "2011-03-01 00:00:00 ----- ['Africa', 'Asia', 'Europe']\n",
      "2011-04-01 00:00:00 ----- ['Africa', 'Europe', 'Americas', 'Asia', 'NA']\n",
      "2011-05-01 00:00:00 ----- ['Africa']\n",
      "2011-06-01 00:00:00 ----- ['Europe']\n",
      "2011-07-01 00:00:00 ----- []\n",
      "2011-08-01 00:00:00 ----- ['Oceania', 'Asia', 'Africa']\n",
      "2011-09-01 00:00:00 ----- ['Africa']\n",
      "2011-10-01 00:00:00 ----- []\n",
      "2011-11-01 00:00:00 ----- []\n",
      "2011-12-01 00:00:00 ----- []\n",
      "2012-01-01 00:00:00 ----- ['Africa', 'Asia']\n",
      "2012-02-01 00:00:00 ----- ['Africa']\n",
      "2012-03-01 00:00:00 ----- ['Africa', 'Europe']\n",
      "2012-04-01 00:00:00 ----- ['Asia', 'Europe', 'Africa', 'NA']\n",
      "2012-05-01 00:00:00 ----- []\n",
      "2012-06-01 00:00:00 ----- []\n",
      "2012-07-01 00:00:00 ----- ['Africa', 'Asia']\n",
      "2012-08-01 00:00:00 ----- ['Asia']\n",
      "2012-09-01 00:00:00 ----- ['Africa', 'Asia', 'Europe', 'Americas', 'NA', 'Oceania']\n",
      "2012-10-01 00:00:00 ----- ['Africa', 'Europe', 'Americas', 'Oceania', 'Asia']\n",
      "2012-11-01 00:00:00 ----- ['Africa', 'Asia', 'Europe']\n",
      "2012-12-01 00:00:00 ----- ['Africa', 'Asia', 'Europe']\n",
      "2013-01-01 00:00:00 ----- []\n",
      "2013-02-01 00:00:00 ----- []\n",
      "2013-03-01 00:00:00 ----- []\n",
      "2013-04-01 00:00:00 ----- []\n",
      "2013-05-01 00:00:00 ----- []\n",
      "2013-06-01 00:00:00 ----- []\n",
      "2013-07-01 00:00:00 ----- []\n",
      "2013-08-01 00:00:00 ----- []\n",
      "2013-09-01 00:00:00 ----- []\n",
      "2013-10-01 00:00:00 ----- []\n",
      "2013-11-01 00:00:00 ----- []\n",
      "2013-12-01 00:00:00 ----- []\n",
      "2014-01-01 00:00:00 ----- []\n"
     ]
    }
   ],
   "source": [
    "for elem in sorted(list(dateddata)):\n",
    "    #my_list.sort(key=operator.itemgetter(1))\n",
    "    print(elem,'-----',[x for x in [x[0] for i,x in enumerate(sorted(dateddata[elem][1].items(), key=operator.itemgetter(1), reverse=True)) if i <= 6]])\n",
    "    #print(elem,'-----',[x for x in list(dateddata[elem][0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as above per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01 00:01:00 ----- []\n",
      "2009-02-01 00:00:00 ----- []\n",
      "2009-03-01 00:00:00 ----- ['Central America', 'Eastern Asia', 'Northern Africa', 'Western Asia', 'Middle Africa', 'Eastern Africa', 'Southern Asia']\n",
      "2009-04-01 00:00:00 ----- ['South America', 'Eastern Asia', 'Southern Asia', 'Western Asia', 'Northern Africa', 'Central America', 'Western Africa']\n",
      "2009-05-01 00:00:00 ----- ['Western Asia', 'Southern Asia', 'Eastern Asia', 'Middle Africa', 'South-Eastern Asia', 'South America', 'Caribbean']\n",
      "2009-06-01 00:00:00 ----- ['Western Asia', 'Eastern Asia', 'Middle Africa', 'Southern Asia', 'Northern Europe', 'South America', 'Central America']\n",
      "2009-07-01 00:00:00 ----- ['Eastern Asia', 'Southern Asia', 'South America', 'Middle Africa', 'Northern Africa', 'Eastern Africa', 'South-Eastern Asia']\n",
      "2009-08-01 00:00:00 ----- ['Western Asia', 'South America', 'Southern Asia', 'Eastern Asia', 'Middle Africa', 'Central America', 'Western Africa']\n",
      "2009-09-01 00:00:00 ----- ['Western Asia', 'Southern Asia', 'Eastern Asia', 'South America', 'Eastern Africa', 'Middle Africa', 'Western Africa']\n",
      "2009-10-01 00:00:00 ----- ['Southern Asia', 'Western Asia', 'Northern Europe', 'Western Africa', 'Northern Africa', 'South America', 'South-Eastern Asia']\n",
      "2009-11-01 00:00:00 ----- ['Southern Asia', 'Western Asia', 'Eastern Asia', 'Central America', 'South America', 'Western Africa', 'Northern Europe']\n",
      "2009-12-01 00:00:00 ----- ['South America', 'Southern Asia', 'Eastern Asia', 'Eastern Africa', 'Caribbean', 'South-Eastern Asia', 'Middle Africa']\n",
      "2010-01-01 00:00:00 ----- ['South America', 'Eastern Asia', 'Southern Asia', 'Western Asia', 'Caribbean', 'Western Africa', 'South-Eastern Asia']\n",
      "2010-02-01 00:00:00 ----- ['Western Asia', 'Northern Europe', 'South America', 'Caribbean', 'Middle Africa', 'Southern Asia', 'South-Eastern Asia']\n",
      "2010-03-01 00:00:00 ----- ['Western Asia', 'Northern Europe', 'Southern Asia', 'Central America', 'Eastern Asia', 'South America', 'Caribbean']\n",
      "2010-04-01 00:00:00 ----- ['Western Asia', 'Southern Asia', 'Northern Europe', 'Northern Africa', 'Western Europe', 'Eastern Europe', 'South-Eastern Asia']\n",
      "2010-05-01 00:00:00 ----- ['Western Asia', 'Eastern Asia', 'Southern Asia', 'Caribbean', 'South America', 'South-Eastern Asia', 'Central America']\n",
      "2010-06-01 00:00:00 ----- ['Western Asia', 'Southern Asia', 'South America', 'Northern Africa', 'Western Europe', 'Northern Europe', 'Middle Africa']\n",
      "2010-07-01 00:00:00 ----- ['Southern Asia', 'Western Asia', 'Caribbean', 'Western Europe', 'South America', 'South-Eastern Asia', 'Central America']\n",
      "2010-08-01 00:00:00 ----- ['Western Asia', 'Caribbean', 'Southern Asia', 'South America', 'Middle Africa', 'Eastern Asia', 'Western Africa']\n",
      "2010-09-01 00:00:00 ----- ['Western Asia', 'Southern Asia', 'South America', 'South-Eastern Asia', 'Eastern Asia', 'Caribbean', 'Western Europe']\n",
      "2010-10-01 00:00:00 ----- ['Western Asia', 'South America', 'Eastern Asia', 'Caribbean', 'Southern Asia', 'Northern Africa', 'Northern Europe']\n",
      "2010-11-01 00:00:00 ----- ['Western Asia', 'Caribbean', 'Southern Asia', 'Eastern Europe', 'Northern Africa', 'Northern Europe', 'South-Eastern Asia']\n",
      "2010-12-01 00:00:00 ----- ['Western Asia', 'Southern Asia', 'Caribbean', 'Eastern Asia', 'Central America', 'Southern Europe', 'Western Africa']\n",
      "2011-01-01 00:00:00 ----- []\n",
      "2011-02-01 00:00:00 ----- []\n",
      "2011-03-01 00:00:00 ----- ['Northern Africa', 'Western Europe', 'Western Asia', 'Southern Asia', 'Middle Africa', 'Eastern Asia']\n",
      "2011-04-01 00:00:00 ----- ['Northern Africa', 'Northern Europe', 'Central America', 'Western Asia']\n",
      "2011-05-01 00:00:00 ----- ['Northern Africa']\n",
      "2011-06-01 00:00:00 ----- ['Northern Europe']\n",
      "2011-07-01 00:00:00 ----- []\n",
      "2011-08-01 00:00:00 ----- ['Southern Asia', 'Southern Africa', 'Australia and New Zealand', 'Western Asia', 'Middle Africa', 'Northern Africa']\n",
      "2011-09-01 00:00:00 ----- ['Northern Africa']\n",
      "2011-10-01 00:00:00 ----- []\n",
      "2011-11-01 00:00:00 ----- []\n",
      "2011-12-01 00:00:00 ----- []\n",
      "2012-01-01 00:00:00 ----- ['Northern Africa', 'Western Asia', 'Eastern Africa']\n",
      "2012-02-01 00:00:00 ----- ['Northern Africa']\n",
      "2012-03-01 00:00:00 ----- ['Northern Europe', 'Western Europe', 'Northern Africa', 'Western Africa']\n",
      "2012-04-01 00:00:00 ----- ['Northern Africa', 'Southern Europe', 'Eastern Asia', 'Western Europe', 'Western Asia']\n",
      "2012-05-01 00:00:00 ----- []\n",
      "2012-06-01 00:00:00 ----- []\n",
      "2012-07-01 00:00:00 ----- ['Northern Africa', 'Western Asia']\n",
      "2012-08-01 00:00:00 ----- ['Western Asia']\n",
      "2012-09-01 00:00:00 ----- ['Northern Africa', 'Western Asia', 'South-Eastern Asia', 'Southern Asia', 'Southern Europe', 'Polynesia', 'Middle Africa']\n",
      "2012-10-01 00:00:00 ----- ['Northern Africa', 'Melanesia', 'Western Europe', 'Northern Europe', 'Central America', 'Western Asia']\n",
      "2012-11-01 00:00:00 ----- ['Northern Africa', 'Southern Asia', 'Northern Europe', 'Western Asia']\n",
      "2012-12-01 00:00:00 ----- ['Northern Africa', 'Western Europe', 'Western Asia']\n",
      "2013-01-01 00:00:00 ----- []\n",
      "2013-02-01 00:00:00 ----- []\n",
      "2013-03-01 00:00:00 ----- []\n",
      "2013-04-01 00:00:00 ----- []\n",
      "2013-05-01 00:00:00 ----- []\n",
      "2013-06-01 00:00:00 ----- []\n",
      "2013-07-01 00:00:00 ----- []\n",
      "2013-08-01 00:00:00 ----- []\n",
      "2013-09-01 00:00:00 ----- []\n",
      "2013-10-01 00:00:00 ----- []\n",
      "2013-11-01 00:00:00 ----- []\n",
      "2013-12-01 00:00:00 ----- []\n",
      "2014-01-01 00:00:00 ----- []\n"
     ]
    }
   ],
   "source": [
    "for elem in sorted(list(dateddata)):\n",
    "    #my_list.sort(key=operator.itemgetter(1))\n",
    "    print(elem,'-----',[x for x in [x[0] for i,x in enumerate(sorted(dateddata[elem][2].items(), key=operator.itemgetter(1), reverse=True)) if i <= 6]])\n",
    "    #print(elem,'-----',[x for x in list(dateddata[elem][0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusions:\n",
    "* It is easy to see how the attention swifted between months to different countries according to main events that occured during the specific period. For example:\n",
    "  * 2009 was Aghanistan\n",
    "  * 2010 was Haiti\n",
    "  * 2011-2012 was the Arab Spring and Lybia\n",
    "* The focus of emails concentrated on regions in conflict of different nature. Those regions that were considered in serious conflict that involved USA received more attention. LatinAmerica was first, then regions around Aghanistan and then the region where the Arab Spring occurred.\n",
    "* The messaging also involved the mentioning of countries that were considered allies to USA, with a strong focus on Israel and Oman, but also Western Europe and OTAN, as well as Pakistan, India, Japan, South Korea and Australia. In Latinamerica, Central America, Peru, Colombia and Brazil seemed to receive special mentioning.\n",
    "* This results are just a first overview and far to be correct. The reasons of lack of veracity are not only because of the data itself but also the methodology used here. Specifically about the methodology, bear in mind that:\n",
    "  * there is an unknown number of false positives as well as false negatives when approaching the search using this parsing and the modified country dataset used for the search.\n",
    "  * some emails contained relevant information about countries in other forms: personalities (which could be directed related to an event in a country or region), or cities/capitals instead of countries (eg. Benghazi)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
